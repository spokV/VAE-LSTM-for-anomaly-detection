{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.8.5-final","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"colab":{"name":"sepsis-lstm.ipynb","provenance":[],"collapsed_sections":[]}},"cells":[{"cell_type":"code","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"id":"1tNuUtEq0mml"},"source":["import os\n","import pandas as pd\n","import numpy as np\n","import math\n","#from keras.models import Sequential\n","#from keras.layers import Dense\n","#from keras.layers import LSTM"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true,"id":"nIEOAP0N0mm0"},"source":["path = './sepsis'\n","path_a = './sepsis/training_setB'\n","path_b = './sepsis/training_setA'\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# This function plots a dataset with the train/test split and known anomalies\n","# Relies on helper function load_data()\n","\n","def process_and_save_specified_dataset(dataset, y_scale=5, save_file=False):\n","    t, t_unit, readings, idx_anomaly, idx_split, save_dir = load_data(dataset)\n","    \n","    # print(\"readings: \", readings.shape)\n","    # split into training and test sets\n","    training = readings[idx_split[0]:idx_split[1]]\n","    t_train = t[idx_split[0]:idx_split[1]]\n","\n","    readings_normalised = np.zeros(readings.shape, dtype=float)\n","    # print(\"shape: !!! \", training.shape)\n","\n","    # normalise by training mean and std \n","    train_m = training.mean(axis=0) #np.mean(training[:channel])\n","    train_std = training.std(axis=0) #np.std(training[:channel])\n","    print(\"\\nTraining set mean is {}\".format(train_m))\n","    print(\"Training set std is {}\".format(train_std))\n","    # print(\"readings_normalised: \", readings_normalised[:,1])\n","    channels_num = len(train_m)\n","    for channel in range(channels_num):\n","        readings_normalised[:,channel] = (readings[:,channel] - train_m[channel]) / train_std[channel]\n","    \n","    training = readings_normalised[idx_split[0]:idx_split[1]]\n","    if idx_split[0] == 0:\n","        test = readings_normalised[idx_split[1]:]\n","        t_test = t[idx_split[1]:] - idx_split[1]\n","        idx_anomaly_test = np.asarray(idx_anomaly) - idx_split[1]\n","    else:\n","        test = [readings_normalised[:idx_split[0]], readings_normalised[idx_split[1]:]]\n","        t_test = [t[:idx_split[0]], t[idx_split[1]:] - idx_split[1]]\n","        idx_anomaly_split = np.squeeze(np.argwhere(np.asarray(idx_anomaly)>idx_split[0]))\n","        idx_anomaly_test = [np.asarray(idx_anomaly[:idx_anomaly_split[0]]), \n","                            np.asarray(idx_anomaly[idx_anomaly_split[0]:]) - idx_split[1]]\n","    print(\"Anomaly indices in the test set are {}\".format(idx_anomaly_test))\n","    \n","    if save_file:\n","        #save_dir = './datasets/NAB-known-anomaly/'\n","        np.savez(save_dir+dataset+'.npz', t=t, t_unit=t_unit, readings=readings, idx_anomaly=idx_anomaly,\n","                    idx_split=idx_split, training=training, test=test, train_m=train_m, train_std=train_std,\n","                    t_train=t_train, t_test=t_test, idx_anomaly_test=idx_anomaly_test)\n","        print(\"\\nProcessed time series are saved at {}\".format(save_dir+dataset+'.npz'))\n","    else:\n","        print(\"\\nProcessed time series are not saved.\")\n","    \n","    # plot the whole normalised sequence\n","    fig, axs = plt.subplots(channels_num, 1, figsize=(18, 4), edgecolor='k')\n","    fig.subplots_adjust(hspace=.4, wspace=.4)\n","    # print(\"readings_normalised: \", readings_normalised[:,channel])\n","    # axs = axs.ravel()\n","    # for i in range(4):\n","    for channel in range(channels_num):\n","        axs[channel].plot(t, readings_normalised[:,channel])\n","        if idx_split[0] == 0:\n","            axs[channel].plot(idx_split[1]*np.ones(20), np.linspace(-y_scale,y_scale,20), 'b--')\n","        else:\n","            for i in range(2):\n","                axs[channel].plot(idx_split[i]*np.ones(20), np.linspace(-y_scale,y_scale,20), 'b--')\n","        for j in range(len(idx_anomaly)):\n","            axs[channel].plot(idx_anomaly[j]*np.ones(20), np.linspace(-y_scale,y_scale,20), 'r--')\n","        #     axs.plot(data[:,1])\n","        axs[channel].grid(True)\n","        axs[channel].set_xlim(0, len(t))\n","        axs[channel].set_ylim(-y_scale, y_scale)\n","        axs[channel].set_xlabel(\"timestamp (every {})\".format(t_unit))\n","        axs[channel].set_ylabel(\"normalised readings\")\n","        axs[channel].set_title(\"{} dataset\\n(normalised by train mean {:.2f} and std {:.2f})\".format(dataset, train_m[channel], train_std[channel]))\n","        axs[channel].legend(('data', 'train test set split', 'anomalies'))\n","    \n","    return t, readings_normalised"]},{"cell_type":"code","metadata":{"trusted":true,"id":"vsxQU-fR0mm1"},"source":["y_train = []\n","X_train = []\n","\n","for i in os.listdir(path_a):\n","    data = pd.read_csv(path_a+'/'+i,sep = '|')\n","    data.drop(['EtCO2','Fibrinogen', 'Unit1', 'Unit2', 'BaseExcess', 'DBP', 'Hct', 'Hgb', 'PTT', 'WBC', 'pH','HCO3','FiO2', 'PaCO2', 'Platelets', 'Magnesium',  'Phosphate',  'Potassium', 'Bilirubin_total',  'TroponinI','SaO2', 'AST','BUN', 'Alkalinephos', 'Bilirubin_direct','Glucose','Lactate', 'Calcium',  'Chloride', 'Creatinine' ],axis = 1,inplace = True)\n","\n","    data.dropna(thresh=data.shape[1]*0.40,how='all',inplace = True)\n","    La_1 = data['SepsisLabel'].sum()\n","    if La_1:\n","        y_train.append(1)\n","    else:\n","        y_train.append(0)\n","    data.drop(['SepsisLabel'],axis = 1,inplace = True)\n","    data = data.apply(lambda x: x.fillna(x.median()),axis=0)\n","    data = data.fillna(0)\n","    if len(data) < 40:\n","        Pad = pd.DataFrame({'HR':0.0 ,'O2Sat':0.0, 'Temp':0.0 , 'SBP':0.0, 'MAP':0.0, 'Resp':0.0, 'Age':0.0, 'Gender': 0 ,'HospAdmTime':0.0, 'ICULOS':0}, index =[item for item in range(0,40-len(data))])\n","        data = pd.concat([Pad, data]).reset_index(drop = True)\n","    elif len(data) >40:\n","        data = data[len(data)-40::1]\n","    data = data.values\n","    X_train.append(data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"jmKTfQfw0mm3"},"source":["for i in os.listdir(path_b):\n","    data = pd.read_csv(path_b+'/'+i,sep = '|')\n","    data.drop(['EtCO2','Fibrinogen', 'Unit1', 'Unit2', 'BaseExcess', 'DBP', 'Hct', 'Hgb', 'PTT', 'WBC', 'pH','HCO3','FiO2', 'PaCO2', 'Platelets', 'Magnesium',  'Phosphate',  'Potassium', 'Bilirubin_total',  'TroponinI','SaO2', 'AST','BUN', 'Alkalinephos', 'Bilirubin_direct','Glucose','Lactate', 'Calcium',  'Chloride', 'Creatinine' ],axis = 1,inplace = True)\n","\n","    data.dropna(thresh=data.shape[1]*0.40,how='all',inplace = True)\n","    La_1 = data['SepsisLabel'].sum()\n","    if La_1:\n","        y_train.append(1)\n","    else:\n","        y_train.append(0)\n","    data.drop(['SepsisLabel'],axis = 1,inplace = True)\n","    data = data.apply(lambda x: x.fillna(x.median()),axis=0)\n","    data = data.fillna(0)\n","    if len(data) < 40:\n","        Pad = pd.DataFrame({'HR':0.0 ,'O2Sat':0.0, 'Temp':0.0 , 'SBP':0.0, 'MAP':0.0, 'Resp':0.0, 'Age':0.0, 'Gender': 0 ,'HospAdmTime':0.0, 'ICULOS':0}, index =[item for item in range(0,40-len(data))])\n","        data = pd.concat([Pad, data]).reset_index(drop = True)\n","    elif len(data) >40:\n","        data = data[len(data)-40::1]\n","    data = data.values\n","    X_train.append(data)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"aRqmcY_r0mm4"},"source":["X, y = np.array(X_train) , np.array(y_train)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"nRusFzGS0mm5","outputId":"306dbe54-7735-4c2a-e188-501a1e89b385"},"source":["print(len(X))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"u2NQpZHw0mm5","outputId":"5249f0e3-8158-430b-b64c-ac0c4d5e7ee0"},"source":["from sklearn.model_selection import train_test_split\n","\n","\n","X_train_, X_test_, y_train_, y_test_ = train_test_split(X, y, test_size=0.25, random_state=45)\n","print(len(X_train_))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"9AENDuYZ0mm6","outputId":"116aea85-470a-49ce-ca04-dd9087120159"},"source":["print(len(X_train_))\n","print(len(X_test_))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"gRDX7-Na0mm7","outputId":"a62b21e2-05d4-47bc-96dd-e772d7afd2e8"},"source":["model = Sequential()\n","model.add(LSTM(128, input_shape=(40,10), return_sequences = True))\n","model.add(LSTM(256))\n","model.add(Dense(64, activation='relu'))\n","model.add(Dense(1, activation='sigmoid'))\n","model.compile(loss='mean_squared_error', optimizer='adam', metrics=['accuracy'])\n","history = model.fit(X_train_, y_train_, epochs=10, batch_size=32, verbose=1, validation_split=0.2, shuffle=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"1AlfOXGP0mm8","outputId":"3ea9a2d9-1d30-4627-8867-b48c8a12286b"},"source":["import matplotlib.pyplot as plt\n","\n","\n","# Plot training & validation accuracy values\n","plt.plot(history.history['accuracy'])\n","plt.plot(history.history['val_accuracy'])\n","plt.title('Model accuracy')\n","plt.ylabel('Accuracy')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()\n","\n","# Plot training & validation loss values\n","plt.plot(history.history['loss'])\n","plt.plot(history.history['val_loss'])\n","plt.title('Model loss')\n","plt.ylabel('Loss')\n","plt.xlabel('Epoch')\n","plt.legend(['Train', 'Test'], loc='upper left')\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"zczsCOIc0mm9","outputId":"848aece2-d9c8-4c3f-d16c-95f3b29fb2f7"},"source":["model_json = model.to_json()\n","with open(\"model.json\", \"w\") as json_file:\n","    json_file.write(model_json)\n","# serialize weights to HDF5\n","#model.save_weights(\"model.h5\")\n","print(\"Saved model to disk\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"dNXbRRz80mm9","outputId":"5c5de438-e6a5-4e2e-a03b-19eb2007e12d"},"source":["'''\n","from keras.models import model_from_json\n","# load json and create model\n","json_file = open('/kaggle/working/model.json', 'r')\n","loaded_model_json = json_file.read()\n","json_file.close()\n","loaded_model = model_from_json(loaded_model_json)\n","# load weights into new model\n","loaded_model.load_weights(\"/kaggle/working/model.h5\")\n","print(\"Loaded model from disk\")'''"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"7XxsGcm90mm-"},"source":["#y_pred = loaded_model.predict(X_test_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"a3VsPWKj0mm-"},"source":["y_pred = model.predict(X_test_)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"6p6czOm-0mm_","outputId":"4bb382a5-9530-4de1-af53-cb3745879ceb"},"source":["from collections import Counter\n","from scipy import stats\n","\n","dist = Counter(y)\n","for k in dist:\n","    dist[k] /= len(X)\n","\n","acum = 0\n","bound = {}\n","for i in range(1):\n","    acum += dist[i]\n","    bound[i] = np.percentile(y_pred, acum * 100)\n","print(bound)\n","\n","def classify(x):\n","    if x <= bound[0]:\n","        return 0\n","    else:\n","        return 1\n","    \n","final_pred = np.array(list(map(classify, y_pred)))\n","print(final_pred)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"IBdwBUub0mm_","outputId":"d11f24c8-ee9f-497f-f2cd-bfc7cb9c3e4b"},"source":["from sklearn.metrics import confusion_matrix\n","confusion_matrix(y_test_, final_pred)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"ghByPc1B0mnA","outputId":"ff0ab020-2a6c-411a-8aa7-071d57060fd4"},"source":["uniqueValues, occurCount = np.unique(y_pred, return_counts=True)\n","occurCount"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"4ZbhZFkU0mnA","outputId":"5321ca50-755d-439d-e1b5-85b7ab2afb79"},"source":["uniqueValues, occurCount = np.unique(y_test_, return_counts=True)\n","occurCount"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"trusted":true,"id":"cJA7QA3c0mnB"},"source":[],"execution_count":null,"outputs":[]}]}